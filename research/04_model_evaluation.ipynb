{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed9c22a-538d-475c-b7de-076ebd462849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 19:45:43.233241: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-27 19:45:43.293126: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-27 19:45:44.661137: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9efddc-15fa-45e3-a350-c00609cf0826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lab/Workspace/e2e-mlflow-dvc\n"
     ]
    }
   ],
   "source": [
    "%cd /home/lab/Workspace/e2e-mlflow-dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcef0dad-eccd-4b64-97d8-c5217f579672",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"]=\"https://dagshub.com/diazmanne/e2e-mlflow-dvc.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"]=\"diazmanne\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"]=\"f4e87115c3d98a2ef151c0469443be9738fa35ff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaaf93fd-bc49-475e-9aab-34f5b74ac000",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"artifacts/training/trained_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64d4fee1-c910-4990-a24f-0ba9d0956c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    path_of_model: Path\n",
    "    training_data: Path\n",
    "    test_data: Path  # Added test_data here to match the code usage\n",
    "    all_params: dict\n",
    "    mlflow_uri: str\n",
    "    params_image_size: list\n",
    "    params_batch_size: int\n",
    "\n",
    "# Validation function\n",
    "def validate_evaluation_config(evaluation_config: EvaluationConfig):\n",
    "    print(f\"Model Path: {evaluation_config.path_of_model}\")\n",
    "    print(f\"Training Data Path: {evaluation_config.training_data}\")\n",
    "    print(f\"Test Data Path: {evaluation_config.test_data}\")\n",
    "    print(f\"MLflow URI: {evaluation_config.mlflow_uri}\")\n",
    "    print(f\"Image Size: {evaluation_config.params_image_size}\")\n",
    "    print(f\"Batch Size: {evaluation_config.params_batch_size}\")\n",
    "    print(f\"All Params: {evaluation_config.all_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ce681f5-47e9-4d14-96e3-bba241104457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d344d6ae-9791-466a-a3fa-89a31413be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        # Update paths to match actual data location\n",
    "        self.config[\"evaluation\"][\"training_data_path\"] = \"artifacts/data_ingestion/Chest-CT-Scan-data\"\n",
    "        self.config[\"evaluation\"][\"test_data_path\"] = \"artifacts/data_ingestion/Chest-CT-Scan-data\"\n",
    "        \n",
    "        create_directories([self.config[\"artifacts_root\"]])\n",
    "\n",
    "    def get_evaluation_config(self) -> EvaluationConfig:\n",
    "        eval_config = EvaluationConfig(\n",
    "            path_of_model=Path(self.config[\"evaluation\"][\"trained_model_path\"]),\n",
    "            training_data=Path(self.config[\"evaluation\"][\"training_data_path\"]),\n",
    "            test_data=Path(self.config[\"evaluation\"][\"test_data_path\"]),\n",
    "            mlflow_uri=self.config[\"mlflow_uri\"],\n",
    "            all_params=self.params,\n",
    "            params_image_size=self.params[\"IMAGE_SIZE\"],\n",
    "            params_batch_size=self.params[\"BATCH_SIZE\"]\n",
    "        )\n",
    "        return eval_config\n",
    "\n",
    "# Validation function\n",
    "def validate_evaluation_config(evaluation_config: EvaluationConfig):\n",
    "    print(f\"Model Path: {evaluation_config.path_of_model}\")\n",
    "    print(f\"Training Data Path: {evaluation_config.training_data}\")\n",
    "    print(f\"Test Data Path: {evaluation_config.test_data}\")\n",
    "    print(f\"MLflow URI: {evaluation_config.mlflow_uri}\")\n",
    "    print(f\"Image Size: {evaluation_config.params_image_size}\")\n",
    "    print(f\"Batch Size: {evaluation_config.params_batch_size}\")\n",
    "    print(f\"All Params: {evaluation_config.all_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ebdb82f-e934-434f-bb47-1cb6a43e4083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from urllib.parse import urlparse\n",
    "import logging\n",
    "from cnnClassifier.utils.common import save_json\n",
    "import mlflow.tensorflow  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e6fad2e-3d14-4cec-972e-0ab6eaed926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    def __init__(self, config: EvaluationConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.valid_generator = None\n",
    "        self.score = None\n",
    "        logging.info(\"Evaluation instance created successfully.\")\n",
    "\n",
    "    def _valid_generator(self):  # Fixed method name (removed asterisks)\n",
    "        try:\n",
    "            logging.info(\"Initializing the validation data generator.\")\n",
    "            \n",
    "            datagenerator_kwargs = dict(\n",
    "                rescale=1.0 / 255,\n",
    "                validation_split=0.30\n",
    "            )\n",
    "            \n",
    "            dataflow_kwargs = dict(\n",
    "                target_size=self.config.params_image_size[:-1],\n",
    "                batch_size=self.config.params_batch_size,\n",
    "                interpolation=\"bilinear\",\n",
    "                class_mode='categorical'  # Added class_mode for classification\n",
    "            )\n",
    "            \n",
    "            valid_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                **datagenerator_kwargs\n",
    "            )\n",
    "            \n",
    "            self.valid_generator = valid_datagenerator.flow_from_directory(\n",
    "                directory=str(self.config.training_data),  # Convert Path to string\n",
    "                subset=\"validation\",\n",
    "                shuffle=False,\n",
    "                **dataflow_kwargs\n",
    "            )\n",
    "            logging.info(\"Validation data generator initialized successfully.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error initializing validation generator: {e}\")\n",
    "            raise e\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(path: Path) -> tf.keras.Model:\n",
    "        try:\n",
    "            logging.info(f\"Loading model from: {path}\")\n",
    "            model = tf.keras.models.load_model(str(path))  # Convert Path to string\n",
    "            logging.info(\"Model loaded successfully.\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading model: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def evaluation(self):\n",
    "        try:\n",
    "            logging.info(\"Starting model evaluation.\")\n",
    "            self.model = self.load_model(self.config.path_of_model)\n",
    "            self._valid_generator()\n",
    "            \n",
    "            if self.valid_generator is None:\n",
    "                raise ValueError(\"Validation generator not initialized.\")\n",
    "            \n",
    "            self.score = self.model.evaluate(\n",
    "                self.valid_generator,\n",
    "                steps=len(self.valid_generator)  # Added steps parameter\n",
    "            )\n",
    "            logging.info(f\"Evaluation completed. Loss: {self.score[0]}, Accuracy: {self.score[1]}\")\n",
    "            self.save_score()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during evaluation: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def save_score(self):\n",
    "        try:\n",
    "            logging.info(\"Saving evaluation scores.\")\n",
    "            scores = {\"loss\": float(self.score[0]), \"accuracy\": float(self.score[1])}  # Convert to Python float\n",
    "            save_json(path=Path(\"scores.json\"), data=scores)\n",
    "            logging.info(f\"Scores saved successfully: {scores}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving scores: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def log_into_mlflow(self):\n",
    "        try:\n",
    "            logging.info(\"Logging metrics and model to MLflow.\")\n",
    "            mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "            tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "            \n",
    "            with mlflow.start_run():\n",
    "                mlflow.log_params(self.config.all_params)\n",
    "                mlflow.log_metrics({\n",
    "                    \"loss\": float(self.score[0]),\n",
    "                    \"accuracy\": float(self.score[1])\n",
    "                })\n",
    "                \n",
    "                # Log model using mlflow.tensorflow\n",
    "                logging.info(\"Generating model signature for MLflow logging.\")\n",
    "                input_data = next(iter(self.valid_generator))[0][:5]  # Get sample inputs\n",
    "                predictions = self.model.predict(input_data)\n",
    "                \n",
    "                # Highlighted code\n",
    "                mlflow.tensorflow.log_model(\n",
    "                    self.model,\n",
    "                    \"model\",\n",
    "                    signature=mlflow.models.signature.infer_signature(input_data, predictions)\n",
    "                )\n",
    "                logging.info(\"Model logged to MLflow successfully.\")\n",
    "                print(tf.__version__)  # Instead of tensorflow.keras.__version__\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during MLflow logging: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f831fa-5c55-4a11-bbe4-ff5e8b149993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-27 19:45:49,577: INFO: common: YAML file loaded successfully: config/config.yaml]\n",
      "[2025-01-27 19:45:49,581: INFO: common: YAML file loaded successfully: params.yaml]\n",
      "[2025-01-27 19:45:49,583: INFO: common: Created directory at: artifacts]\n",
      "Model Path: artifacts/training/trained_model.keras\n",
      "Training Data Path: artifacts/data_ingestion/Chest-CT-Scan-data\n",
      "Test Data Path: artifacts/data_ingestion/Chest-CT-Scan-data\n",
      "MLflow URI: https:/dagshub.com/diazmanne/e2e-mlflow-dvc.mlflow\n",
      "Image Size: [224, 224, 3]\n",
      "Batch Size: 16\n",
      "All Params: {'AUGMENTATION': True, 'IMAGE_SIZE': [224, 224, 3], 'BATCH_SIZE': 16, 'INCLUDE_TOP': False, 'EPOCHS': 1, 'CLASSES': 2, 'WEIGHTS': PosixPath('imagenet'), 'LEARNING_RATE': 0.01}\n",
      "[2025-01-27 19:45:49,584: INFO: 4223989291: Evaluation instance created successfully.]\n",
      "[2025-01-27 19:45:49,585: INFO: 4223989291: Starting model evaluation.]\n",
      "[2025-01-27 19:45:49,587: INFO: 4223989291: Loading model from: artifacts/training/trained_model.keras]\n",
      "[2025-01-27 19:45:51,864: INFO: 4223989291: Model loaded successfully.]\n",
      "[2025-01-27 19:45:51,867: INFO: 4223989291: Initializing the validation data generator.]\n",
      "Found 52 images belonging to 2 classes.\n",
      "[2025-01-27 19:45:51,876: INFO: 4223989291: Validation data generator initialized successfully.]\n",
      "1/4 [======>.......................] - ETA: 45s - loss: 18.2775 - accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Initialize configuration\n",
    "    config = ConfigurationManager()\n",
    "    eval_config = config.get_evaluation_config()\n",
    "    \n",
    "    # Validate configuration\n",
    "    validate_evaluation_config(eval_config)\n",
    "    \n",
    "    # Perform evaluation\n",
    "    evaluation = Evaluation(eval_config)\n",
    "    evaluation.evaluation()\n",
    "    \n",
    "    # Log results to MLflow\n",
    "    evaluation.log_into_mlflow()\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"File not found error: {str(e)}\")\n",
    "    print(\"Please check if all the paths in the configuration are correct.\")\n",
    "    raise e\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {type(e).__name__}\")\n",
    "    print(f\"Error message: {str(e)}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdca629-ff77-4b6c-addc-102f6ace8adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
